{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 크롤링을 해봅시다 룰루랄라🎈😁"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "크롤링에 대해 간단하게 설명하면 웹 페이지의 내용을 그대로 추출하는 행위로 스크래핑이라고도 불립니다.\n",
    "\n",
    "파이썬에서는 크롤링을 위해 `BeautifulSoup` 과 `Selenium`을 주로 사용합니다.<br>\n",
    "`BeautifulSoup`은 html 형식의 정적인 웹 페이지를 크롤링 할 때 사용하며, <br>\n",
    "`Selenium`은 javascript로 쓰여진 동적 웹 페이지를 크롤링 할 때 사용합니다. <br>\n",
    "\n",
    "정적인 웹 페이지는 웹 페이지가 로딩되었다면, 더이상 값이 변하지 않는 형태의 웹 페이지를 말합니다.<br>\n",
    "반대로 동적인 웹 페이지는 웹 페이지가 로딩되고나서 URL 주소가 같음에도 사용자의 행동에 따라 페이지 정보가 변하는 웹 페이지를 말합니다.<br> 주로 스크롤을 함에 따라 아래에 내용이 추가로 등장하거나, 토글 스위치 등으로 추가적인 정보를 제공하는 웹 페이지가 동적 웹 페이지에 해당한다고 생각하면 됩니다.<br>\n",
    "<br>\n",
    "뷰티풀수프는 브라우저를 여는 것 없이 주소(url)를 받아와 그 안의 html 내용을 요청하는 방식으로 크롤링하지만,<br>\n",
    "셀레니움은 직접 드라이버를 이용해 웹 브라우저를 동작시켜 직접 키보드로 입력하고, 마우스로 클릭하는 등의 동작을 수행한다는 특징이 있습니다.<br> 또한 뷰티풀수프가 할 수 있는 것에 플러스로 클릭, 검색어 입력, 스크롤 등의 추가적인 조작이 가능한 장점이 있지만 뷰티풀수프에 비해서 느리다는 단점이 있습니다.<br>\n",
    "<br>\n",
    "웹 페이지의 특성에 따라 사용하는 라이브러리가 달라지겠지만, 가장 최고의 방법은 셀레니움을 이용해 클릭/검색어입력/스크롤 등의 행동만 취해주고, 해당 웹페이지 내용 자체는 뷰티풀수프로 긁어오는 것이 가장 빠를 거예요. (필요한 내용이 정적 웹 페이지에 속한다면요)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사전 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "크롤링을 위해서 몇 가지 사전 준비가 필요합니다! <br>\n",
    "이 부분은 다들 스터디 전까지 준비해오세요 \\~\\~\\~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **아나콘다, 주피터 노트북 설치**  \n",
    "[여기](https://annajang.tistory.com/26) 를 참고해서 아나콘다와 주피터노트북을 설치해주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **크롬 설치**    \n",
    "셀레니움 라이브러리를 사용하기 위해서는 크롬 브라우저가 필요합니다.  \n",
    "크롬을 설치해주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **크롬드라이버 설치**  \n",
    "셀레니움 라이브러리를 사용하기 위해서는 크롬드라이버를 설치해야 합니다.  \n",
    "자신 크롬 버전에 맞게 크롬드라이버를 [여기](https://chromedriver.chromium.org/downloads)에서 설치해주세요.  \n",
    "설치된 드라이버파일(exe)은 찾기 쉬운 곳에 두세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*크롬 버전은 크롬 실행 후 우측 상단에 '쩜세개'를 누르고 도움말 > Chrome 정보 를 누르면 확인할 수 있어요*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **주피터 노트북 안에서 설정**<br>\n",
    "주피터 노트북을 설치할 때 기본적으로 깔리는 라이브러리(넘파이, 판다스, 맷플롯립, 사이킷런 등)도 있지만 그렇지 않은 라이브러리들도 있습니다.<br>\n",
    "우리가 이번 시간에 사용할 라이브러리들은 그렇지 않은 라이브러리들이기 때문에 직접 설치해주어야 합니다. 아나콘다 프롬프트에서 설치를 할 수도 있고, 주피터 노트북 내에서 설치를 할 수도 있는데 저희는 주피터 노트북 내에서 직접 설치해주도록 하겠습니다.<br>\n",
    "<br>\n",
    "아래 명령어를 통해서 설치할 수 있어요.<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4 \n",
    "!pip install selenium "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`!` 이 느낌표는 파이썬이 아닌 '명령 프롬프트 자체에서 실행시키겠다'를 의미합니다. 우리가 지금 사용하고 있는 언어는 파이썬이지만 설치는 컴퓨터에 해주어야 하므로 이렇게 해준다고 생각하시면 돼요! <br>\n",
    "한번만 해주면 되므로 설치했다면 주석 처리 해줘서 불필요한 실행을 하지 않도록 합시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "설치가 되었다면 버전을 확인해봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bs4 version : 4.9.3\n",
      "selenium version : 3.141.0\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "import selenium\n",
    "\n",
    "print(f\"bs4 version : {bs4.__version__}\")\n",
    "print(f\"selenium version : {selenium.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "버전이 정상적으로 나오므로 설치가 다 되었다고 볼 수 있겠죠?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 같이 해봅시다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기본 세팅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 라이브러리 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 설정을 위해\n",
    "import os\n",
    "\n",
    "# 웹 페이지 크롤링 라이브러리\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver \n",
    "import requests\n",
    "\n",
    "# 데이터 프레임 생성 및 저장 (xlsx, csv)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 경로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 노트북 경로 설정 (각자 상황에 맞춰서)\n",
    "os.chdir(\"C://Users//whoe9//Desktop/jupyternotebook/\")\n",
    "\n",
    "# 크롬드라이버를 설치한 경로 저장\n",
    "driver_path = \"chromedriver.exe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 경로 설정 잘 되었는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\whoe9\\\\Desktop\\\\jupyternotebook'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 웹 드라이버"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 웹 드라이버란?<br>\n",
    "셀레니움을 이용해 크롤링을 할 때에는 컴퓨터가 크롬이나 파이어폭스 브라우저를 직접 실행시킴으로써 크롤링을 하게 됩니다.<br>\n",
    "이 때 컴퓨터가 직접 브라우저를 열어 크롤링을 하도록 돕는 프로그램이 바로 웹 드라이버입니다. 다들 설치는 미리 해놨죠?! <br>\n",
    "<br>\n",
    "\n",
    "- 웹 드라이버 설정<br>\n",
    "이때 컴퓨터가 브라우저를 실행시키게 하기에 앞서 몇 가지 설정이 필요할 때가 있습니다.<br>\n",
    "이를 `webdriver.ChromeOptions()` 클래스를 통해 인스턴스를 지정해주고, <br>\n",
    "`.add_argument`라는 메소드를 통해 그 인스턴스에 옵션을 추가해줌으로써 다양한 옵션 커스터마이징이 가능해집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chrome_options 인스턴스 지정\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "\n",
    "# 몇몇 옵션들 \n",
    "#crhome_options.add_argument('headless') #headless모드 브라우저가 뜨지 않고 실행됩니다.\n",
    "#crhome_options.add_argument('--window-size= x, y') #실행되는 브라우저 크기를 지정할 수 있습니다.\n",
    "#crhome_options.add_argument('--start-maximized') #브라우저가 최대화된 상태로 실행됩니다.\n",
    "#crhome_options.add_argument('--start-fullscreen') #브라우저가 풀스크린 모드(F11)로 실행됩니다.\n",
    "#crhome_options.add_argument('--blink-settings=imagesEnabled=false') #브라우저에서 이미지 로딩을 하지 않습니다.\n",
    "#crhome_options.add_argument('--mute-audio') #브라우저에 음소거 옵션을 적용합니다.\n",
    "#crhome_options.add_argument('incognito') #시크릿 모드의 브라우저가 실행됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 본격 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 기본 세팅이 끝났으니 본격적으로 웹 페이지 크롤링을 할 시간입니다!<br>\n",
    "크롤링을 하기 위해서는 html 언어의 기본 구조를 이해할 필요가 있는데요. 크롤링을 하기 위한 가장 기본적인 것들만 알아봅시다.\n",
    "\n",
    "0. 웹 페이지의 html 언어를 보기 위해서는 브라우저를 열고 F12를 눌러 개발자 도구를 열어줍니다. (크롬으로 해줍시다.)\n",
    "1. html은 계층적인 구조를 가집니다.\n",
    " - 열린 개발자 도구 화면을 보면 무수하게 많은 미지의 언어들 옆에 화살표가 있습니다.\n",
    " - 이 화살표들은 하위 구조들을 모두 포함하는 상위 클래스입니다.\n",
    "2. 우리가 크롤링하고 싶은 요소의 구조를 알기 위해서는 개발자 도구 좌상단의 버튼을 클릭해주세요.\n",
    "3. 이 버튼을 클릭한 채로 웹 페이지 위에서 마우스를 움직여보면, 각각의 요소에 해당하는 html 구조가 나타납니다.\n",
    "4. 우리는 이걸 이용해서 크롤링을 해볼 거예요!<br>\n",
    "\n",
    "---------------\n",
    "\n",
    "크롤링을 할 때는 주로 css_selector나 xpath를 이용해서 해당 요소에 접근합니다. (저같은 경우에는요)<br>\n",
    "\n",
    "- css_selector 는 쉽게 생각해서 html의 계층 구조를 key 값을 이용해서 요소에 접근하는 방식이고\n",
    "- xpath 는 각각의 요소의 고유한 주소로 접근하는 방식입니다.<br>\n",
    "\n",
    "크롤링이 가능하게 만들기 위해서는 어떤 라이브러리든지 html 상의 규칙을 찾아내서 그걸 코드화 하는 것이 필수입니다.<br>\n",
    "자세한 방법은 직접 해보면서 느껴볼까요?📝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저희는 css_selector를 주로 이용해서 해볼게요.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 네이버 기사 첫 페이지 크롤링 (정적)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 검색어를 직접 지정해봅시다!<br>\n",
    "네이버 뉴스 메인 화면에서 오른쪽 상단에 검색어를 입력하면 페이지가 전환되면서 아래와 같은 주소가 입력되어있습니다.<br>\n",
    "하나 다른 점이 있다면 `query` 오른쪽 부분만 우리가 직접 입력해주어야 한다는 것이죠?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input(\"검색어를 입력: \")\n",
    "url = f\"https://search.naver.com/search.naver?where=news&sm=tab_jum&query={query}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 크롬 드라이버 실행<br>\n",
    "이제 우리가 크롤링 할 웹 페이지가 정해졌습니다.<br>\n",
    "크롤링을 하기 위해 셀레니움의 webdriver 모듈을 이용해 드라이버를 실행시켜줘야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 드라이버 파일(exe, 다운받아놓은것!) 경로를 지정해주고 위에서 설정한 옵션도 추가해줍니다.\n",
    "driver = webdriver.Chrome(driver_path, options=chrome_options)\n",
    "\n",
    "# 아래 get 메소드를 사용하면 크롬 창이 열리면서 네이버 뉴스 검색 화면이 등장할 겁니다.\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 페이지 html 내용 따로 저장하기<br>\n",
    "드라이버는 현재 우리가 지정한 검색어 뉴스 검색 페이지에 머물러 있습니다.<br>\n",
    "여기의 모\\~\\~든 내용을 저장해놓을까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 크롬 화면이 머무르고 있는 페이지의 html 내용 전부를 req 변수에 저장\n",
    "req = driver.page_source\n",
    "\n",
    "# 뷰티풀수프를 이용해 페이지 내용을 html.parser 방법을 이용해서 파싱\n",
    "soup = BeautifulSoup(req, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 뉴스 기사 리스트만 따로 저장<br>\n",
    "지금 soup에 담긴 html 내용은 우리가 필요하지 않은 요소들까지 전부 포함하고 있습니다.  \n",
    "우리는 뉴스 기사 리스트만 필요하니까 뉴스 기사를 포함하고 있는 부분만을 따로 저장합시다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직접 F12를 눌러 selector를 카피해보기\n",
    "news_list = soup.select('#main_pack > section > div > div.group_news > ul > li')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기사 하나하나를 크롤링  \n",
    "뉴스 기사 리스트 내용 전체가 `news_list`에 담겨있습니다.  \n",
    "여기서 이제 우리가 필요한 내용만 뽑아냅시다.  \n",
    " - 기사 제목, 신문사, 발행 일자, 대략적인 내용  \n",
    "정도만 있으면 되겠죠?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 페이지 내용만 크롤링 하기\n",
    "for news in news_list:\n",
    "    \n",
    "    # 뉴스 제목\n",
    "    title = news.select_one('.news_tit') # 클래스로 접근\n",
    "    print(f\"제목 : {title.text}\")\n",
    "    \n",
    "    # 신문사\n",
    "    news_company = news.select_one('.news_info > .info_group > a') # 클래스로 접근 후 태그 이름으로 접근\n",
    "    print(f\"신문사 : {news_company.text}\")\n",
    "    \n",
    "    # 일자\n",
    "    '''\n",
    "    뉴스 리스트를 보면, 신문사 옆에 A18면 1단 처럼 실제로 발행된 신문에서의 위치가 포함된 경우가 존재합니다.\n",
    "    이때 이 위치 또한 span이라는 tag name으로 묶여있어 위에서 select_one을 사용한다면, 일자가 아닌 기사 위치가 크롤링 됩니다.\n",
    "    따라서 select를 통해 span 전체를 크롤링 해준 후, 일자 부분만 따로 떼와야 합니다.\n",
    "    이때 일자는 항상 뒤에 오므로 -1으로 인덱싱 해서 일자를 추출해주면 됩니다.\n",
    "    '''\n",
    "    date = news.select('.news_info > .info_group > span')\n",
    "    print(f\"일자 : {date[-1].text}\")\n",
    "    \n",
    "    #요약 내용\n",
    "    contents = news.select_one('.news_dsc')\n",
    "    print(f\"요약 내용 : {contents.text}\")\n",
    "    \n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 페이지를 변경해가며 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "페이지를 변경해가면서 크롤링을 하기 위해서는 크게 두 가지 방법을 찾을 수 있습니다.<br>\n",
    "1. URL 상에서 페이지 번호가 변하는 규칙을 찾아내서 URL을 변경하거나\n",
    "2. 웹 페이지 상에서 페이지 번호를 변경하는 버튼을 찾아내서 클릭하거나<br>\n",
    "\n",
    "이렇게 두 가지 방법이 존재합니다. 결론부터 말하자면 둘 다 모두 셀레니움으로 가능하며, 두 번째 방법은 셀레니움으로만 가능합니다. 셀레니움은 쉽게 말해서 우리가 마우스로 하는 행위들을 컴퓨터가 자동으로 하게 해준다고 생각하면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 방법은 URL이 페이지가 변할 때마다 규칙을 갖고 변하는지(=코드화 할 수 있는지)를 찾아내는 것이며, 두 번째 방법은 따로 다른 방법이 필요하지 않습니다. 버튼의 html 요소를 찾아내고 셀레니움 라이브러리를 이용해 클릭 명령을 내려주면 돼요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. URL 상에서 페이지 번호가 변하는 규칙을 찾아내서 URL을 변경하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 링크들은 같은 검색어에서 페이지만 바꾼 링크들입니다. 다른 것들을 찾아볼까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://search.naver.com/search.naver?where=news&sm=tab_pge&query=%EC%BD%94%EB%A1%9C%EB%82%98&sort=0&photo=0&field=0&pd=0&ds=&de=&cluster_rank=68&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:all,a:all&start=1\n",
    "\n",
    "\n",
    "  \n",
    " https://search.naver.com/search.naver?where=news&sm=tab_pge&query=%EC%BD%94%EB%A1%9C%EB%82%98&sort=0&photo=0&field=0&pd=0&ds=&de=&cluster_rank=95&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:all,a:all&start=11\n",
    " \n",
    " \n",
    "https://search.naver.com/search.naver?where=news&sm=tab_pge&query=%EC%BD%94%EB%A1%9C%EB%82%98&sort=0&photo=0&field=0&pd=0&ds=&de=&cluster_rank=109&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:all,a:all&start=21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cluster_rank`와 `start`뒤에 붙는 것들만 다른 것 같습니다. <br>\n",
    "이 중에서 `cluster_rank`는 특별한 규칙이 없어 보이는데 페이지를 띄우고 크롤링 하는 데에 영향을 줄까요?<br>\n",
    "직접 링크를 주소창에 입력하고 `start` 뒤에 붙는 것만 변경해보면 크게 변화하는 것은 없는 것을 알 수 있습니다. <br>\n",
    "따라서 `start`가 페이지를 변화시키는 파트라고 생각할 수 있겠죠?<br>\n",
    "우리는 이 `start` 뒤에 붙는 파트만 바꾸면서 페이지를 변경하면서 웹 페이지 크롤링을 할 수 있게 됩니다.<br>\n",
    "규칙은 `(페이지숫자-1) * 10 + 1` 인 것 같네요. 더 단순하게 생각하면 1페이지 이후 10씩 더해집니다. <br>\n",
    "그럼 이걸 직접 코드로 적용해서 페이지까지 변경하면서 크롤링을 해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다만 여기서는 뷰티풀수프만 이용해서 크롤링 하려고 합니다.<br>\n",
    "셀레니움과 같이 쓰려면 위에서 했던 것처럼 페이지별로 웹페이지를 열어주고, 거기에 해당하는 정보를 긁어오는 식으로 해야하는데 <br>\n",
    "그렇게 하면 웹 페이지를 열고 닫는 데에 불필요한 시간이 걸리니까 그냥 뷰티풀수프로만 기사를 크롤링하려고 합니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원리는 똑같습니다. 아까 위에서 한 페이지 내의 모든 기사를 크롤링 하는 반복문을 짰죠?<br>\n",
    "그걸 페이지 수만큼 반복해주면 되니까 페이지 수를 변화시키는 반복문을 바깥에다가 씌워주면 됩니다~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 검색어는 성균관대\n",
    "query = \"성균관대\"\n",
    "\n",
    "# 첫 페이지부터 시작\n",
    "page_num = 1\n",
    "\n",
    "# 크롤링 할 끝 페이지 지정 (여기서는 10페이지 까지만 합시다)\n",
    "last_page_num = 10\n",
    "\n",
    "# 페이지를 변화시키는 반복문\n",
    "while True:\n",
    "    \n",
    "    # 현재 페이지는 몇 페이지?\n",
    "    print(f\"현재 페이지 : {(page_num-1)//10+1}\")\n",
    "    \n",
    "    # 검색할 링크\n",
    "    url = f\"https://search.naver.com/search.naver?where=news&sm=tab_pge&query={query}&sort=0&photo=0&field=0&pd=0&ds=&de=&cluster_rank=385&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:all,a:all&start={page_num*10+1}\"\n",
    "    response = requests.get(url) # driver.get(url).page_source과 거의 같습니다. \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    \n",
    "    # 현재 페이지에서 기사 10개 크롤링 하기\n",
    "    news_list = soup.select('#main_pack > section > div > div.group_news > ul > li')\n",
    "    for news in news_list:\n",
    "    \n",
    "        # 뉴스 제목\n",
    "        title = news.select_one('.news_tit') \n",
    "        print(f\"제목 : {title.text}\")\n",
    "            \n",
    "        # 신문사\n",
    "        news_company = news.select_one('.news_info > .info_group > a') \n",
    "        print(f\"신문사 : {news_company.text}\")\n",
    "    \n",
    "        # 일자\n",
    "        date = news.select('.news_info > .info_group > span')\n",
    "        print(f\"일자 : {date[-1].text}\")\n",
    "    \n",
    "        #요약 내용\n",
    "        contents = news.select_one('.news_dsc')\n",
    "        print(f\"요약 내용 : {contents.text}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    print(\"-\"*30)\n",
    "    if page_num == 10*(last_page_num-1)+1: # 마지막 페이지에서 크롤링을 다하면 종료\n",
    "        break\n",
    "        \n",
    "    page_num += 10 # page_num = page_num + 10 과 같습니다.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. 웹 페이지 상에서 페이지 번호를 변경하는 버튼을 찾아내서 클릭하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "드디어 동적 웹 페이지를 다루는 셀레니움의 진가를 알 수 있는 시간입니다! <br>\n",
    "네이버 기사는 url을 바꾸면서 페이지를 바꾸는 것 외에도 하단의 페이지를 넘기는 버튼을 클릭함으로써 페이지를 변경할 수 있습니다.<br>\n",
    "이 방법의 장점은 네이버 기사 한정으로 끝 페이지가 몇인지 몰라도 '다음' 버튼이 나오지 않을 때까지 같은 작업을 수행할 수 있다는 겁니다. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "과정은 모두 같고 페이지를 변경하는 과정이 셀레니움의 클릭을 이용해서 진행된다는 차이점만 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색어 입력\n",
    "query = input(\"검색어를 입력: \")\n",
    "\n",
    "# url 지정\n",
    "url =  f\"https://search.naver.com/search.naver?where=news&sm=tab_pge&query={query}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = webdriver.ChromeOptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롬드라이버 실행\n",
    "driver = webdriver.Chrome(driver_path, options=chrome_options)\n",
    "driver.implicitly_wait(10) # 웹 페이지 로딩 대기 시간 최대 10초. 10초 안에 다 로딩이 되었다면 그냥 다음 코드 실행\n",
    "\n",
    "# 해당 url에 접근 (우리가 크롤링 할 초기 웹 페이지)\n",
    "driver.get(url)\n",
    "req = driver.page_source\n",
    "soup = BeautifulSoup(req, 'html.parser') # 페이지 내용을 html.parser 방법을 이용해서 파싱\n",
    "\n",
    "# selector를 이용해 뉴스 리스트에 접근\n",
    "news_list = soup.select('#main_pack > section > div > div.group_news > ul > li')\n",
    "\n",
    "# 시작 페이지 번호\n",
    "page_num = 1\n",
    "\n",
    "# 크롤링 종료할 페이지 번호\n",
    "last_page_num = 10\n",
    "\n",
    "while True:\n",
    "    \n",
    "    print(f\"현재 페이지 : {page_num}\")\n",
    "    for news in news_list:\n",
    "        # 뉴스 제목\n",
    "        title = news.select_one('.news_tit') # 클래스로 접근\n",
    "        print(f\"제목 : {title.text}\")\n",
    "\n",
    "        # 신문사\n",
    "        news_company = news.select_one('.news_info > .info_group > a') # 클래스로 접근 후 태그 이름으로 접근\n",
    "        print(f\"신문사 : {news_company.text}\")\n",
    "\n",
    "        # 일자\n",
    "        '''\n",
    "        뉴스 리스트를 보면, 신문사 옆에 A18면 1단 처럼 실제로 발행된 신문에서의 위치가 포함된 경우가 존재합니다.\n",
    "        이때 이 위치 또한 span이라는 tag name으로 묶여있어 위에서 select_one을 사용한다면, 일자가 아닌 기사 위치가 크롤링 됩니다.\n",
    "        따라서 select를 통해 span 전체를 크롤링 해준 후, 일자 부분만 따로 떼와야 합니다.\n",
    "        이때 일자는 항상 뒤에 오므로 -1으로 인덱싱 해서 일자를 추출해주면 됩니다.\n",
    "        '''\n",
    "        date = news.select('.news_info > .info_group > span')\n",
    "        print(f\"일자 : {date[-1].text}\")\n",
    "\n",
    "        #요약 내용\n",
    "        contents = news.select_one('.news_dsc')\n",
    "        print(f\"요약 내용 : {contents.text}\")\n",
    "\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    # 반복문 종료 조건\n",
    "    if page_num == last_page_num:\n",
    "        break\n",
    "    \n",
    "    # 다음 페이지 번호 계산\n",
    "    page_num += 1\n",
    "\n",
    "    # 다음 버튼 클릭\n",
    "    next_button = driver.find_element_by_css_selector('#main_pack > div.api_sc_page_wrap > div > a.btn_next')\n",
    "    next_button.click()\n",
    "    \n",
    "# 다 됐으면 크롬브라우저 종료\n",
    "driver.quit()\n",
    "\n",
    "# driver.close() 는 현재 선택된 탭을 종료하는 함수입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 이렇게 url만 바꾸면서, 웹 브라우저를 열지 않고 빠르게 크롤링이 가능한 뷰티풀수프를 냅두고 굳이 셀레니움을 사용할 이유는 없겠죠?<br>\n",
    "시간이 더 걸리니까요.<br>\n",
    "그러면 셀레니움을 더 잘 써먹을 수 있는 웹 페이지를 크롤링 해봅시다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 동적 웹 페이지 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[카카오맵](https://map.kakao.com/) 을 크롤링 해봅시다.<br>\n",
    "카카오맵을 들어가서 검색어를 입력하거나 페이지를 바꾸는 등 특정한 동작을 취해도 URL이 변하지 않는 것을 볼 수 있습니다. <br>\n",
    "이러한 경우가 바로 동적 웹 페이지에 해당합니다.<br>\n",
    "카카오 맵을 이용해 장소 정보를 크롤링 해볼까요?<br>\n",
    "아까 초반에 셀레니움을 이용해 검색어를 입력하거나 클릭, 스크롤 등의 동작을 할 수 있다고 했죠? <br>\n",
    "이런 행동들을 셀레니움을 이용해 해볼게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = webdriver.ChromeOptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동적 웹 페이지에서 사용자 행동을 위한 모듈 입력\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(driver_path, options=chrome_options)\n",
    "driver.implicitly_wait(10) # 웹 페이지 로딩 대기 시간 최대 10초. 10초 안에 다 로딩이 되었다면 그냥 다음 코드 실행\n",
    "driver.get('https://map.kakao.com/')\n",
    "\n",
    "# 검색어 저장\n",
    "query = \"혜화 맛집\"\n",
    "\n",
    "# 검색어 입력하는 박스로 이동\n",
    "search_box = driver.find_element_by_css_selector('.box_searchbar > input')\n",
    "# 검색어 입력 후 엔터\n",
    "search_box.send_keys(query, Keys.ENTER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [더보기 버튼을 클릭이 아닌 엔터키를 입력하는 이유](https://blog.naver.com/PostView.nhn?isHttpsRedirect=true&blogId=kiddwannabe&logNo=221430636045)  \n",
    "- 다음 페이지로 넘기기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 장소 더보기 버튼 = 2페이지로 이동 (click()이 아닌 엔터키를 입력함. 이유는 위 링크!)\n",
    "more_button = driver.find_element_by_css_selector('#info\\.search\\.place\\.more')\n",
    "more_button.send_keys(Keys.ENTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1페이지로 이동\n",
    "page_button = driver.find_element_by_css_selector('#info\\.search\\.page\\.no1')\n",
    "page_button.send_keys(Keys.ENTER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원래는 셀레니움만 이용해서 해보려고 했는데 자꾸 오류가 나서 페이지를 바꾸는 것만 셀레니움을 쓰고, 내용을 크롤링하는 건 뷰티풀수프를 쓸게요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kakao_map_result = pd.DataFrame(columns=['장소명', '카테고리', '평점', '주소', '상태', '영업시간'])\n",
    "last_page_num = 19\n",
    "page_button_num = 1\n",
    "true_page_num = 1 \n",
    "\n",
    "while True:\n",
    "    \n",
    "    print(f\"현재 {true_page_num} 페이지 크롤링 중 ....\")\n",
    "    \n",
    "    # 현재 페이지의 모든 검색 리스트 정보 저장\n",
    "    time.sleep(1)\n",
    "    req = driver.page_source\n",
    "    soup = BeautifulSoup(req, 'html.parser')\n",
    "    search_list = soup.select('#info\\.search\\.place\\.list > li.PlaceItem.clickArea')\n",
    "    \n",
    "    # 현재 페이지의 모든 정보 크롤링\n",
    "    for i, restaurant in enumerate(search_list):\n",
    "        \n",
    "        print(f\"-----현재 {i+1}번째 장소 크롤링 중\")\n",
    "        \n",
    "        # 식당 이름\n",
    "        res_name_tmp = restaurant.select_one('div.head_item.clickArea > strong > a.link_name')\n",
    "        #res_name.append(res_name_tmp.text)\n",
    "\n",
    "        # 카테고리\n",
    "        category_tmp = restaurant.select_one('div.head_item.clickArea > span')\n",
    "        #category.append(category_tmp.text)\n",
    "\n",
    "        # 평점\n",
    "        rating_tmp = restaurant.select_one('div.rating.clickArea > span.score > em')\n",
    "        #rating.append(rating_tmp.text)\n",
    "\n",
    "        # 도로명주소\n",
    "        addr_tmp = restaurant.select_one('div.info_item > div.addr > p:nth-child(1)')\n",
    "        #addr.append(addr_tmp.text)\n",
    "\n",
    "        # 영업중/브레이크타임\n",
    "        status_tmp = restaurant.select_one('div.info_item > div.openhour > p > span')\n",
    "        #status.append(status_tmp.text)\n",
    "\n",
    "        # 영업시간/브레이크타임시간\n",
    "        openhour_tmp = restaurant.select_one('div.info_item > div.openhour > p > a')\n",
    "        #openhour.append(openhour_tmp.text)\n",
    "        \n",
    "        \n",
    "        # 데이터 프레임에 추가\n",
    "        kakao_map_result = kakao_map_result.append({'장소명' : res_name_tmp.text, '카테고리' : category_tmp.text, '평점' : rating_tmp.text,\n",
    "                                 '주소' : addr_tmp.text, '상태' : status_tmp.text, '영업시간' : openhour_tmp.text}, ignore_index=True)\n",
    "    \n",
    "        \n",
    "    # 지정된 페이지까지만 크롤링\n",
    "    if true_page_num == last_page_num:\n",
    "        break\n",
    "    \n",
    "    # 페이지 번호가 5의 배수일 경우에 다음 페이지 버튼 클릭\n",
    "    # 페이지 버튼 번호 초기화\n",
    "    if page_button_num == 5:\n",
    "        next_button = driver.find_element_by_css_selector('#info\\.search\\.page\\.next')\n",
    "        next_button.send_keys(Keys.ENTER)\n",
    "        page_button_num = 1\n",
    "        true_page_num += 1\n",
    "        \n",
    "    # 5의 배수가 아니라면 페이지 번호를 클릭\n",
    "    else:\n",
    "        page_button_num += 1\n",
    "        true_page_num += 1\n",
    "        page_button = driver.find_element_by_css_selector(f'#info\\.search\\.page\\.no{page_button_num}')\n",
    "        page_button.send_keys(Keys.ENTER)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **결과 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kakao_map_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
